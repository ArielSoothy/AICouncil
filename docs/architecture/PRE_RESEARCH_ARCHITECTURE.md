# Pre-Research Architecture: Why Models Don't Research Autonomously

**Created**: November 2025
**Status**: Design Document
**Related**: UNIFIED_DEBATE_ENGINE.md, domain-framework.ts

---

## Executive Summary

This document explains why AI models don't autonomously call web search tools during debates, despite having tools available. It provides architectural guidance for implementing a pre-research stage that ensures consistent, high-quality research across all debate sessions.

---

## The Core Discovery

### Observed Behavior

Server logs showed that web search tools were enabled but never called:

```
=== OPENAI DEBUG ===
useWebSearch: true
Tools passed: none
OpenAI: Native web search enabled

=== OPENAI SUCCESS ===
üîç Step finished: { toolCalls: 0, toolResults: 0 }  <-- Model chose not to search
```

**Result**: "Found 0 sources" for all agents despite tools being available.

### Root Cause

Models have web search tools available but **choose not to call them**. This is by design in how LLMs are trained.

---

## Why Models Don't Research Autonomously

### 1. Training Objective Conflict

LLMs are trained to **answer directly**, not to "think about what to research first."

Their optimization function rewards:
- Providing coherent, helpful responses
- Using knowledge from training data
- Being confident in existing knowledge

Tool-calling is a **secondary behavior** that competes with the primary "answer the question" instinct.

### 2. Prompt Design Issue

Current debate prompts instruct models to:

```
"CITE SPECIFIC EVIDENCE" - Each claim must be supported
"Use phrases like: Based on..., Research indicates..."
```

This tells models to **cite evidence they already have**, not to **go search for new evidence**.

Models interpret this as: "Use your training knowledge to cite sources."

### 3. Expert Persona Framing

The debate prompts frame models as domain experts:

```
"As The Analyst, you are opening this important debate..."
"Your expertise areas: [technical analysis, market data, risk assessment]"
```

This frames the model as an **expert who already knows things**, not a researcher who needs to gather information first.

### 4. Tool Usage Cost Perception

Models learn during training that:
- Tool calls add latency
- Users prefer quick responses
- Conversational contexts reward directness

Without explicit instruction that "you MUST search first," models default to answering directly.

### 5. Multi-Step Reasoning Overhead

To autonomously search, a model must:
1. Recognize information gap
2. Decide to search
3. Formulate search query
4. Execute tool call
5. Parse results
6. Integrate into response

Each step is a decision point where the model can "give up" and just answer directly.

---

## Existing Architecture Analysis

The codebase already has the **correct architecture** for pre-research:

### Domain Framework (`types/domain-framework.ts`)

```typescript
export interface DomainFramework {
  // Research is DESIGNED to be separate from debate
  researchPromptGenerator: (context: StructuredContext) => string;
  preferredSources?: string[];  // Pre-defined sources
  defaultResearchMode?: ResearchMode;
  additionalSearchQueries?: (context: StructuredContext) => string[];
}
```

### Query Analyzer (`lib/heterogeneous-mixing/query-analyzer.ts`)

```typescript
export function analyzeQuery(query: string): QueryAnalysis {
  // BEFORE calling models, determine if search is needed
  return {
    requiresWebSearch: boolean;
    requiresMultiStep: boolean;
    primaryType: QueryType;
    complexity: 'low' | 'medium' | 'high' | 'very-high';
  }
}
```

### Context Extractor (`lib/web-search/context-extractor.ts`)

```typescript
// System generates search queries, NOT the model
generateSearchQueries(
  context: ExtractedContext,
  originalQuery: string,
  role: 'analyst' | 'critic' | 'synthesizer',
  round: number
): string[]
```

### Research Cache (`lib/trading/research-cache.ts`)

```typescript
// Already implemented for trading - can extend to debates
export class ResearchCache {
  async getCache(cacheKey: string): Promise<CachedData | null>
  async setCache(cacheKey: string, data: ResearchData, ttlMinutes: number): Promise<void>
}
```

---

## Pre-Research vs Per-Agent Research Comparison

| Aspect | Per-Agent Research (Tool-Based) | Pre-Research Stage |
|--------|--------------------------------|-------------------|
| **Control** | Model decides IF and WHAT to search | System decides, model receives |
| **Reliability** | Unpredictable (0% to 100%) | Consistent (100% when enabled) |
| **Query Quality** | Model-dependent, often suboptimal | Optimized, role-specific queries |
| **Caching** | Complex (per-model, per-query) | Simple (per-query) |
| **Cost** | Redundant searches (each model searches) | Single search, shared context |
| **Latency** | Serial (search ‚Üí think ‚Üí respond) | Parallel (search while preparing) |
| **Evidence Quality** | Varies by model | Standardized, verified sources |
| **Debugging** | Hard (black box) | Easy (logged search queries) |

---

## Recommended Architecture

### Data Flow

```
User Query
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Query Analyzer      ‚îÇ  ‚Üê Detect if web search needed
‚îÇ  (query-analyzer.ts) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº requiresWebSearch = true
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Context Extractor   ‚îÇ  ‚Üê Generate optimized search queries
‚îÇ  (context-extractor) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº searchQueries[]
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Web Search Service  ‚îÇ  ‚Üê Execute searches (DuckDuckGo/Native)
‚îÇ  (web-search/)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº searchResults[]
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Research Cache      ‚îÇ  ‚Üê Cache results for future queries
‚îÇ  (research-cache.ts) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº cachedResearch
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Prompt Builder      ‚îÇ  ‚Üê Inject as "RESEARCH CONTEXT"
‚îÇ  (debate-prompts.ts) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº enrichedPrompt
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Agent Debate        ‚îÇ  ‚Üê Models debate with evidence
‚îÇ  (agent-system.ts)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementation Strategy

#### Phase 1: Pre-Research Integration

1. **In `agent-system.ts`**:
   - Check `analyzeQuery(query).requiresWebSearch`
   - If true, generate search queries via `ContextExtractor`
   - Execute search with DuckDuckGo (reliable fallback)
   - Cache results

2. **In `debate-prompts.ts`**:
   - Add `RESEARCH CONTEXT` section to prompts
   - Include search results as pre-gathered evidence
   - Update instructions: "Use the research provided below"

#### Phase 2: Caching Layer

3. **Extend `research-cache.ts`**:
   - Add debate query ‚Üí search results caching
   - Smart TTL based on query type (current events = short, historical = long)

#### Phase 3: Keep Tools as Fallback

4. **Round 2 Enhancement**:
   - Keep native web search tools for Round 2
   - Models can search for specific counter-evidence
   - Pre-research covers breadth, tool-search covers depth

---

## Cache Key Strategy

For debate queries:

```typescript
const cacheKey = `debate-${hashQuery(query)}-${round}`;

// Example:
// "Should I buy TSLA?" ‚Üí "debate-abc123-round1"
// Different rounds may need different research focus
```

For trading queries (existing):

```typescript
const cacheKey = `${symbol}-${timeframe}`;
// "TSLA-swing" different from "TSLA-day"
```

---

## TTL Strategy

Based on query type detection:

| Query Type | TTL | Reasoning |
|------------|-----|-----------|
| `current-events` | 15 min | Information changes rapidly |
| `factual` | 4 hr | Facts are stable |
| `analytical` | 1 hr | Analysis may need fresh data |
| `comparative` | 2 hr | Comparisons are moderately stable |
| `technical` | 4 hr | Technical docs change slowly |
| Default | 1 hr | Safe middle ground |

---

## Prompt Injection Format

New prompt structure with pre-research:

```
üé≠ MULTI-AGENT DEBATE - OPENING STATEMENT (ROUND 1)

Query: ${query}

üìö RESEARCH CONTEXT (Pre-gathered evidence for your analysis):
${researchContext}

Sources consulted: ${sources.join(', ')}

---

As ${agent.name}, you are opening this important debate.

Your role is to:
1. **USE THE RESEARCH PROVIDED** - Cite from the evidence above
2. **Take a STRONG position** - Be confident in your analysis
3. **Present 3-4 key arguments** with evidence from the research
...
```

---

## Trade-offs and Considerations

### Advantages of Pre-Research

1. **Consistent Quality**: Every debate has researched evidence
2. **Cost Effective**: Single search shared by all agents
3. **Cacheable**: Same query = instant cached results
4. **Debuggable**: Clear log of what was searched
5. **Controllable**: System decides search strategy

### Disadvantages

1. **Less Model "Intelligence"**: Models don't learn to research
2. **Pre-determined Focus**: May miss tangential relevant info
3. **Additional Latency**: Search adds ~2-5s before debate
4. **Complexity**: Another system component to maintain

### Mitigation

- Keep tool-based search as Round 2 fallback
- Allow models to request additional searches
- Cache aggressively to minimize latency impact
- Log all searches for debugging

---

## Success Metrics

After implementation, measure:

1. **Evidence Citation Rate**: % of responses that cite provided research
2. **Cache Hit Rate**: % of debates using cached research
3. **Latency Impact**: Additional time from pre-research
4. **User Satisfaction**: Quality improvement in debate responses
5. **Cost Per Debate**: Reduction from shared research

---

## Related Documentation

- `docs/architecture/UNIFIED_DEBATE_ENGINE.md` - Overall debate architecture
- `docs/features/TRADING_ENHANCEMENTS.md` - Trading research caching
- `docs/guides/RESEARCH_CACHE_TESTING.md` - Cache testing guide
- `types/domain-framework.ts` - Framework plugin interface
- `lib/research/research-coordinator.ts` - Modular research decision-making

---

## Academic Research Supporting This Pattern

### Key Papers (2023-2025)

The pre-research architecture follows established best practices from academic literature:

#### 1. Multi-Agent Collaboration Survey (arxiv, January 2025)
- **Source**: [arxiv.org/abs/2501.06322](https://arxiv.org/abs/2501.06322)
- **Key Finding**: "Centralized Planning, Decentralized Execution" (CPDE) is optimal
- **Relevance**: Research/planning done centrally, agents execute independently
- **Application**: Our pre-research phase = centralized planning

#### 2. Du et al. - Multiagent Debate for Factuality (2023)
- **Source**: [arxiv.org/abs/2305.14325](https://arxiv.org/abs/2305.14325)
- **Key Finding**: Cross-validation between agents reduces hallucinations by **40%**
- **Relevance**: All agents must work from the **same factual basis**
- **Application**: Shared pre-research ensures consistent evidence

#### 3. MAD Strategies - ICML 2024
- **Source**: [proceedings.mlr.press/v235/smit24a.html](https://proceedings.mlr.press/v235/smit24a.html)
- **Key Finding**: 15% accuracy improvement with proper debate structure
- **Relevance**: Modulating agreement intensity between agents improves outcomes
- **Application**: Consistent research enables meaningful cross-validation

#### 4. Agentic RAG Architecture (IBM)
- **Source**: [ibm.com/think/topics/agentic-rag](https://www.ibm.com/think/topics/agentic-rag)
- **Key Insight**: "RAG is the research assistant providing grounded answers; an AI Agent is the project manager executing a plan."
- **Relevance**: Separate retrieval phase from generation phase
- **Application**: Pre-research (retrieval) ‚Üí debate (generation)

### Best Practice Summary

> **"Research ONCE, debate MANY times"**

Academic consensus supports:
1. Centralized research phase (all agents receive same evidence)
2. Tool-based retrieval separated from LLM reasoning
3. Cross-validation more meaningful when data is consistent
4. Reduced redundancy and cost with shared research

### Implementation (December 2024)

This pattern is now implemented in `lib/research/research-coordinator.ts`:

```typescript
// Create coordinator with configuration
const coordinator = createResearchCoordinator({
  enableWebSearch: true,
  hasCentralizedResearch: true,  // Set when conductGeneralResearch ran
  centralizedSourceCount: 7      // Number of sources from centralized research
})

// Coordinator decides: skip redundant DuckDuckGo if centralized research exists
const decision = coordinator.makeResearchDecision(agentCapabilities)
// Result: { shouldRunDuckDuckGo: false, reason: "Centralized research already complete" }
```

---

## Conclusion

The observation that models don't autonomously call web search tools is **expected behavior**, not a bug. LLMs are optimized to answer directly, and tool-calling is a secondary behavior.

The correct architectural pattern is:
1. **System decides** if research is needed (query analysis)
2. **System executes** research (pre-research stage)
3. **System provides** evidence to models (prompt injection)
4. **Models debate** based on provided evidence (their strength)

This separates concerns properly:
- **System**: Research gathering, caching, query optimization
- **Models**: Analysis, reasoning, synthesis, debate

The existing codebase already has the infrastructure for this pattern. Implementation requires connecting the pieces.
